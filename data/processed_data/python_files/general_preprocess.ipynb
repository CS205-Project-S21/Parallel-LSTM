{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0f9520ba11fdc1099356e05d27a34ed870b487e4078e45fe1cddd0198c8b5354c",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "def main():\n",
    "    stockprice_rawdata_path = '../../stock_price_Energy/data/price_HFC.csv'\n",
    "    news_rawdata_path = '../../news_Energy/data/news_large.csv'\n",
    "\n",
    "    spark = SparkSession.builder.master('local[2]').appName('GeneralDataProcess').getOrCreate()\n",
    "\n",
    "    window_duration = '11 day'\n",
    "    slide_duration = '1 day'\n",
    "    num_datapoints = 22\n",
    "\n",
    "    startdate = datetime.date(2016, 3, 31)\n",
    "    enddate = datetime.date(2021, 4, 16)\n",
    "\n",
    "    ##########################################  1. For StockPrice Data ###################################################\n",
    "    ## read csv\n",
    "    df = spark.read.csv(stockprice_rawdata_path, header=True)\n",
    "\n",
    "    # copy date to create a new time check colume\n",
    "    df = df.withColumn('Date', f.to_timestamp(df['Date'], 'yyyy-MM-dd'))\n",
    "    df2 = df.withColumn('TrueDate', f.to_timestamp(df['Date'], 'yyyy-MM-dd'))\\\n",
    "        .withColumn('TrueDate_existent', f.to_timestamp(df['Date'], 'yyyy-MM-dd'))\n",
    "    df2 = df2.select('TrueDate', 'Type', 'Price', 'Truedate_existent')\n",
    "\n",
    "    # df2 looks like:\n",
    "    # +-------------------+------+------------------+-------------------+\n",
    "    # |           TrueDate|  Type|             Price|  Truedate_existent|\n",
    "    # +-------------------+------+------------------+-------------------+\n",
    "    # |2016-03-31 00:00:00|  Open| 2.309999942779541|2016-03-31 00:00:00|\n",
    "    # |2016-03-31 00:00:00|Closed| 2.700000047683716|2016-03-31 00:00:00|\n",
    "    # |2016-04-01 00:00:00|  Open|2.5999999046325684|2016-04-01 00:00:00|\n",
    "    # |2016-04-01 00:00:00|Closed| 2.630000114440918|2016-04-01 00:00:00|\n",
    "    # |2016-04-04 00:00:00|  Open| 2.630000114440918|2016-04-04 00:00:00|\n",
    "    # |2016-04-04 00:00:00|Closed| 2.630000114440918|2016-04-04 00:00:00|\n",
    "\n",
    "    # Fill the missing row with help of a full dateframe\n",
    "    full_dict = {'TrueDate': [], 'Type': []}\n",
    "    cdate = startdate\n",
    "    while cdate <= enddate:\n",
    "        full_dict['TrueDate'].extend([cdate, cdate])\n",
    "        full_dict['Type'].extend(['Open', 'Closed'])\n",
    "        cdate += datetime.timedelta(days=1)\n",
    "    df_ref_pd = pd.DataFrame(full_dict)\n",
    "    df_ref = spark.createDataFrame(df_ref_pd)\n",
    "    df3 = df_ref.join(df2, on=['TrueDate', 'Type'], how='left_outer')\n",
    "    df3 = df3.na.fill(value=0, subset=[\"Price\", \"Truedate_existent\"])\n",
    "    df3 = df3.withColumn('TrueDate', f.to_timestamp(df3['TrueDate'], 'yyyy-MM-dd'))\n",
    "\n",
    "    # df3 looks like:\n",
    "    # +-------------------+------+------------------+-------------------+\n",
    "    # |           TrueDate|  Type|             Price|  Truedate_existent|\n",
    "    # +-------------------+------+------------------+-------------------+\n",
    "    # |2016-03-31 00:00:00|  Open| 2.309999942779541|2016-03-31 00:00:00|\n",
    "    # |2016-03-31 00:00:00|Closed| 2.700000047683716|2016-03-31 00:00:00|\n",
    "    # |2016-04-01 00:00:00|  Open|2.5999999046325684|2016-04-01 00:00:00|\n",
    "    # |2016-04-01 00:00:00|Closed| 2.630000114440918|2016-04-01 00:00:00|\n",
    "    # |2016-04-02 00:00:00|  Open|              null|               null|\n",
    "    # |2016-04-02 00:00:00|Closed|              null|               null|\n",
    "\n",
    "    # Forward-filling and Backward-filling Using Window Functions\n",
    "    window_ff = Window.orderBy('TrueDate').rowsBetween(-sys.maxsize, 0)\n",
    "    window_bf = Window.orderBy('TrueDate').rowsBetween(0, sys.maxsize)\n",
    "\n",
    "    # create series containing the filled values\n",
    "    read_last = f.last(df3['Price'], ignorenulls=True).over(window_ff)\n",
    "    readtime_last = f.last(df3['Truedate_existent'], ignorenulls=True).over(window_ff)\n",
    "\n",
    "    read_next = f.first(df3['Price'], ignorenulls=True).over(window_bf)\n",
    "    readtime_next = f.first(df3['Truedate_existent'], ignorenulls=True).over(window_bf)\n",
    "\n",
    "    # add columns to the dataframe\n",
    "    df_filled = df3.withColumn('readvalue_ff', read_last)\\\n",
    "        .withColumn('readtime_ff', readtime_last)\\\n",
    "        .withColumn('readvalue_bf', read_next)\\\n",
    "        .withColumn('readtime_bf', readtime_next)\n",
    "\n",
    "    # Price interpolation between all empty time\n",
    "    df_filled_temp = df_filled.withColumn('if_open', f.when(f.col('Type') == 'Open', 1).otherwise(0))\n",
    "    df_filled2 = df_filled_temp.withColumn('Price_interpol', f.when(f.col('readtime_bf') == f.col('readtime_ff'), f.col('Price'))\\\n",
    "                                           .otherwise((f.col('readvalue_bf') - f.col('readvalue_ff'))\\\n",
    "                                                      / (f.col('readtime_bf').cast(\"long\") - f.col('readtime_ff').cast(\"long\") - 43200)\\\n",
    "                                                      * (f.col('TrueDate').cast(\"long\") - f.col('readtime_ff').cast(\"long\") - 43200 * f.col('if_open')) + f.col('readvalue_ff')))\n",
    "    df4 = df_filled2.select('TrueDate', 'Type', 'Price_interpol')\n",
    "\n",
    "    # df4 looks like:\n",
    "    # +-------------------+------+------------------+\n",
    "    # |           TrueDate|  Type|    Price_interpol|\n",
    "    # +-------------------+------+------------------+\n",
    "    # |2016-03-31 00:00:00|  Open| 2.309999942779541|\n",
    "    # |2016-03-31 00:00:00|Closed| 2.700000047683716|\n",
    "    # |2016-04-01 00:00:00|  Open|2.5999999046325684|\n",
    "    # |2016-04-01 00:00:00|Closed| 2.630000114440918|\n",
    "    # |2016-04-02 00:00:00|  Open| 2.630000114440918|\n",
    "    # |2016-04-02 00:00:00|Closed| 2.630000114440918|\n",
    "\n",
    "    # use the following to combine open and close into an array\n",
    "    w = Window.partitionBy('TrueDate').orderBy(f.desc('Type'))\n",
    "    df5 = df4.withColumn(\n",
    "        'Open_Close', f.collect_list('Price_interpol').over(w))\\\n",
    "        .groupBy('TrueDate').agg(f.max('Open_Close').alias('Open_Close'))\n",
    "\n",
    "    # Create a time window, and collect to form a larger array\n",
    "    df6 = df5.orderBy(\"TrueDate\").groupBy(f.window('TrueDate', window_duration, slide_duration))\\\n",
    "        .agg(f.collect_list('Open_Close'))\\\n",
    "        .withColumnRenamed('collect_list(Open_Close)', 'StockPrice')\n",
    "    df6 = df6.withColumn('StockPrice', f.flatten(df6['StockPrice']).cast(\"array<double>\")).sort(\"window\")\n",
    "\n",
    "    @f.udf(ArrayType(DoubleType()))\n",
    "    def normalize(x):\n",
    "        \"\"\"\n",
    "        Normalize the input to the range between 0 and 1\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        x_normalized = ((x - np.min(x)) / (np.max(x) - np.min(x))).tolist()\n",
    "        return x_normalized\n",
    "\n",
    "    df6 = df6.withColumn('StockPrice', normalize(df6['StockPrice']))\n",
    "\n",
    "    # Kick out rows with less than num_datapoints data points\n",
    "    df6 = df6.withColumn('array_length', f.size(\"StockPrice\"))\n",
    "\n",
    "    '''\n",
    "    df6 looks like\n",
    "    +--------------------+--------------------+------------+\n",
    "    |              window|          StockPrice|array_length|\n",
    "    +--------------------+--------------------+------------+\n",
    "    |{2016-03-20 20:00...|          [0.0, 1.0]|           2|\n",
    "    |{2016-03-21 20:00...|[0.20535692998320...|           4|\n",
    "    |{2016-03-22 20:00...|[0.28685878331826...|           6|\n",
    "    |{2016-03-23 20:00...|[0.35319750114912...|           8|\n",
    "    |{2016-03-24 20:00...|[0.38194427887582...|          10|\n",
    "    |{2016-03-25 20:00...|[0.38194427887582...|          12|\n",
    "    '''\n",
    "    \n",
    "    df6 = df6.filter(df6['array_length'] == num_datapoints).select(['window', 'StockPrice'])\n",
    "\n",
    "    # df6 looks like:\n",
    "    # +--------------------+--------------------+\n",
    "    # |              window|          StockPrice|\n",
    "    # +--------------------+--------------------+\n",
    "    # |{2016-03-30 20:00...|[0.0, 0.500000152...|\n",
    "    # |{2016-03-31 20:00...|[0.0, 0.061224916...|\n",
    "    # |{2016-04-01 20:00...|[0.0, 0.0, 0.0, 0...|\n",
    "    # |{2016-04-02 20:00...|[0.0, 0.0, 0.0, 0...|\n",
    "    # |{2016-04-03 20:00...|[0.0, 0.0, 0.5434...|\n",
    "    # |{2016-04-04 20:00...|[0.0, 0.499999574...|\n",
    "    df6.show()\n",
    "    return df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-62-e03214bc1c18>\", line 127, in normalize\n  File \"<__array_function__ internals>\", line 5, in amin\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 2830, in amin\n    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nValueError: zero-size array to reduction operation minimum which has no identity\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-deed9433c1d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-e03214bc1c18>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# |{2016-04-03 20:00...|[0.0, 0.0, 0.5434...|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# |{2016-04-04 20:00...|[0.0, 0.499999574...|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mdf6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-62-e03214bc1c18>\", line 127, in normalize\n  File \"<__array_function__ internals>\", line 5, in amin\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 2830, in amin\n    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nValueError: zero-size array to reduction operation minimum which has no identity\n"
     ]
    }
   ],
   "source": [
    "df_r = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1853"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "df_r.filter(df_r['array_length'] == f.size(\"StockPrice\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-49-08faf321b18f>\", line 127, in normalize\n  File \"<__array_function__ internals>\", line 5, in amin\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 2830, in amin\n    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nValueError: zero-size array to reduction operation minimum which has no identity\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-cc5ce017108a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStockPrice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-49-08faf321b18f>\", line 127, in normalize\n  File \"<__array_function__ internals>\", line 5, in amin\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 2830, in amin\n    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n  File \"/Users/lihongzhang/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nValueError: zero-size array to reduction operation minimum which has no identity\n"
     ]
    }
   ],
   "source": [
    "df_r.select(f.size(df_r.StockPrice)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}