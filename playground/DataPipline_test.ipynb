{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stockprice_rawdata_path = '../data/stock_price/data/alldate_RIOT.csv'\n",
    "news_rawdata_path = '../data/news/data/news_large.csv'\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('GeneralDataProcess').getOrCreate()\n",
    "\n",
    "window_duration = '11 day'\n",
    "slide_duration = '1 day'\n",
    "num_datapoints = 22\n",
    "\n",
    "startdate = datetime.date(2016,3,31)\n",
    "enddate = datetime.date(2021,4,16)\n",
    "\n",
    "##########################################  1. For StockPrice Data ###################################################\n",
    "## read csv\n",
    "df = spark.read.csv(stockprice_rawdata_path, header=True)\n",
    "df = df.withColumn('Date', f.to_timestamp(df['Date'], 'yyyy-MM-dd'))\n",
    "\n",
    "@f.udf\n",
    "def combine(opening_price, closing_price):\n",
    "    return [opening_price, closing_price]\n",
    "\n",
    "df = df.withColumn('Open_Close', combine(df['Open'], df['Close']))\n",
    "df = df.withColumn(\"Open_Close_new\", f.split(f.regexp_replace(\"Open_Close\", r\"(^\\[)|(\\]$)\", \"\"), \", \").cast(\"array<double>\"))\n",
    "\n",
    "\n",
    "\n",
    "df = df.orderBy('Date').groupBy(f.window('Date', window_duration, slide_duration)) \\\n",
    "    .agg(f.collect_list('Open_Close_new')) \\\n",
    "    .withColumnRenamed('collect_list(Open_Close_new)', 'sliding_window')\n",
    "\n",
    "# flatten\n",
    "df = df.withColumn('sliding_window', f.flatten(df['sliding_window']))\n",
    "\n",
    "@f.udf(ArrayType(DoubleType()))\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize the input to the range between 0 and 1\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x_normalized = ((x - np.min(x)) / (np.max(x) - np.min(x))).tolist()\n",
    "    return x_normalized\n",
    "\n",
    "df = df.withColumn('sliding_window', normalize(df['sliding_window']))\n",
    "df1 = df.withColumn('array_length', f.size(\"sliding_window\"))\n",
    "df1 = df1.filter((df1.array_length == 22)).select('window', 'sliding_window').withColumnRenamed(\"sliding_window\", \"StockPrice\")\n",
    "# df1 looks like:\n",
    "# +--------------------+--------------------+\n",
    "# |              window|          StockPrice|\n",
    "# +--------------------+--------------------+\n",
    "# |{2016-03-30 20:00...|[0.0, 0.500000152...|\n",
    "# |{2016-03-31 20:00...|[0.0, 0.061224916...|\n",
    "# |{2016-04-01 20:00...|[0.0, 0.0, 0.0, 0...|\n",
    "# |{2016-04-02 20:00...|[0.0, 0.0, 0.0, 0...|\n",
    "# |{2016-04-03 20:00...|[0.0, 0.0, 0.5434...|\n",
    "# |{2016-04-04 20:00...|[0.0, 0.499999574...|\n",
    "\n",
    "\n",
    "##########################################  2. For sentiment analysis ###################################################\n",
    "df2 = spark.read.csv(news_rawdata_path, header=True, escape='\"')\n",
    "df2 = df2.withColumn('time', f.to_timestamp(df2['time'], 'yyyy-MM-dd HH:mm:ss'))\n",
    "df2 = df2.withColumn('hour', f.hour(f.col('time')))\n",
    "df2 = df2.withColumn('Day', f.to_date(df2['time'], format='yyyy-MM-dd'))\n",
    "\n",
    "@f.udf\n",
    "def connect_string(a, b):\n",
    "    return a+' '+b\n",
    "\n",
    "df2 = df2.withColumn('AllText', connect_string(connect_string(df2['title'], df2['description']), df2['content']))\n",
    "df3 = df2.select('Day', 'hour', 'Alltext')\n",
    "df3 = df3.withColumn('TrueDate', when(df3.hour < 9, df3['Day'])\n",
    "                                .when(df3.hour >= 16 , f.date_add(df3['Day'], 1))\n",
    "                                .otherwise(df3['Day']))\n",
    "df3 = df3.withColumn('Type', when(df3.hour < 9, 'Open')\n",
    "                                .when(df3.hour >= 16 , 'Open')\n",
    "                                .otherwise('Close'))\n",
    "\n",
    "# calculate sentiment scores for title, description and content\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "@f.udf(returnType=DoubleType())\n",
    "def calculate_sentiment_score(text):\n",
    "    score = analyzer.polarity_scores(text)['compound']\n",
    "    return score\n",
    "\n",
    "df4 = df3.withColumn('score', calculate_sentiment_score(df3['Alltext']))\n",
    "df4 = df4.groupBy(['TrueDate', 'Type']).agg(f.avg(\"score\").alias(\"AverageScore\"))\n",
    "# df5 = df4.orderBy(\"TrueDate\", f.desc(\"Type\")) # We should avoid any unnecessary sort or orderBy in pipeline\n",
    "# df4 look like:\n",
    "# +----------+-----+-------------------+\n",
    "# |  TrueDate| Type|       AverageScore|\n",
    "# +----------+-----+-------------------+\n",
    "# |2021-03-31| Open| 0.6719461538461545|\n",
    "# |2021-04-03| Open| 0.2891708333333335|\n",
    "# |2016-07-05| Open|             0.9879|\n",
    "# |2021-03-29|Close| 0.3657036496350363|\n",
    "# |2021-04-06| Open|  0.646568817204301|\n",
    "# |2016-10-05| Open|             0.9509|\n",
    "# |2016-05-27| Open|              0.999|\n",
    "# |2021-04-14|Close|-0.3307480769230771|\n",
    "# We need to find a way to fill in the missing rows with 0\n",
    "\n",
    "# Fill the missing row with help of a full dateframe\n",
    "full_dict = {'TrueDate':[], 'Type':[]}\n",
    "cdate = startdate\n",
    "while cdate <= enddate:\n",
    "    full_dict['TrueDate'].extend([cdate, cdate])\n",
    "    full_dict['Type'].extend(['Open', 'Close'])\n",
    "    cdate += datetime.timedelta(days=1)\n",
    "df_ref_pd = pd.DataFrame(full_dict)\n",
    "df_ref = spark.createDataFrame(df_ref_pd)\n",
    "df5 = df_ref.join(df4, on=['TrueDate', 'Type'], how='left_outer')\n",
    "df5 = df5.na.fill(value=0,subset=[\"AverageScore\"])\n",
    "# df5 looks like:\n",
    "# +----------+-----+------------+\n",
    "# |  TrueDate| Type|AverageScore|\n",
    "# +----------+-----+------------+\n",
    "# |2016-03-31| Open|         0.0|\n",
    "# |2016-03-31|Close|         0.0|\n",
    "# |2016-04-01| Open|         0.0|\n",
    "# |2016-04-01|Close|         0.0|\n",
    "# |2016-04-02| Open|     -0.1307|\n",
    "# |2016-04-02|Close|         0.0|\n",
    "# |2016-04-03| Open|         0.0|\n",
    "# |2016-04-03|Close|         0.0|\n",
    "# |2016-04-04| Open|      0.9878|\n",
    "# |2016-04-04|Close|         0.0|\n",
    "# |2016-04-05| Open|     -0.1027|\n",
    "\n",
    "# use the following to combine open and close into an array\n",
    "w = Window.partitionBy('TrueDate').orderBy(f.desc('Type'))\n",
    "df6 = df5.withColumn(\n",
    "            'Open_Close', f.collect_list('AverageScore').over(w)\n",
    "        )\\\n",
    "        .groupBy('TrueDate').agg(f.max('Open_Close').alias('Open_Close'))\n",
    "\n",
    "# Create a time window, and collect to form a larger array\n",
    "df7 = df6.orderBy(\"TrueDate\").groupBy(f.window('TrueDate', window_duration, slide_duration)) \\\n",
    "    .agg(f.collect_list('Open_Close')) \\\n",
    "    .withColumnRenamed('collect_list(Open_Close)', 'NewsScore')\n",
    "df7 = df7.withColumn('NewsScore', f.flatten(df7['NewsScore'])).sort(\"window\")\n",
    "\n",
    "# Kick out rows with less than num_datapoints data points \n",
    "df7 = df7.withColumn('array_length', f.size(\"NewsScore\"))\n",
    "df7 = df7.filter((df7.array_length == num_datapoints)).select(['window', 'NewsScore'])\n",
    "# df7.sort(window) looks like:\n",
    "# +--------------------+--------------------+\n",
    "# |              window|           NewsScore|\n",
    "# +--------------------+--------------------+\n",
    "# |{2016-03-30 20:00...|[0.0, 0.0, 0.0, 0...|\n",
    "# |{2016-03-31 20:00...|[0.0, 0.0, -0.130...|\n",
    "# |{2016-04-01 20:00...|[-0.1307, 0.0, 0....|\n",
    "# |{2016-04-02 20:00...|[0.0, 0.0, 0.9878...|\n",
    "# |{2016-04-03 20:00...|[0.9878, 0.0, -0....|\n",
    "# |{2016-04-04 20:00...|[-0.1027, 0.0, 0....|\n",
    "# |{2016-04-05 20:00...|[0.0, 0.0, 0.1104...|\n",
    "# |{2016-04-06 20:00...|[0.1104, 0.0, 0.0...|\n",
    "\n",
    "\n",
    "##########################################  3. Combine df1 and df7 ################################################\n",
    "df_final = df1.join(df7, on=['window'], how='left_outer')\n",
    "\n",
    "# df_final.orderBy('window') looks like:\n",
    "# +--------------------+--------------------+--------------------+\n",
    "# |              window|          StockPrice|           NewsScore|\n",
    "# +--------------------+--------------------+--------------------+\n",
    "# |{2016-03-30 20:00...|[0.0, 0.500000152...|[0.0, 0.0, 0.0, 0...|\n",
    "# |{2016-03-31 20:00...|[0.0, 0.061224916...|[0.0, 0.0, -0.130...|\n",
    "# |{2016-04-01 20:00...|[0.0, 0.0, 0.0, 0...|[-0.1307, 0.0, 0....|\n",
    "# |{2016-04-02 20:00...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.9878...|\n",
    "# |{2016-04-03 20:00...|[0.0, 0.0, 0.5434...|[0.9878, 0.0, -0....|\n",
    "# |{2016-04-04 20:00...|[0.0, 0.499999574...|[-0.1027, 0.0, 0....|\n",
    "# |{2016-04-05 20:00...|[0.63157815484152...|[0.0, 0.0, 0.1104...|\n",
    "# |{2016-04-06 20:00...|[0.05263151290346...|[0.1104, 0.0, 0.0...|\n",
    "# |{2016-04-07 20:00...|[0.21052605161384...|[0.0, 0.0, 0.0, 0...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_pd= df_final.orderBy(\"window\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_pd.to_csv('../data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "spider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
