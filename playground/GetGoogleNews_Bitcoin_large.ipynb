{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"../utils/\")\n",
    "from fetchgooglenews import GoogleNews\n",
    "from fetchcontextweb import fetch_context_web\n",
    "import datetime\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = datetime.date(2016,3,31)\n",
    "enddate = datetime.date(2021,4,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_periods(startdate, enddate, length=30, shift=-1):\n",
    "    totaldays = (enddate - startdate).days + 1\n",
    "    num_periods = math.ceil(totaldays/length)\n",
    "    output_dates = []\n",
    "    for i in range(0, num_periods):\n",
    "        if i < num_periods-1:\n",
    "            leftdate = startdate + datetime.timedelta(days=i*length)\n",
    "            rightdate = startdate + datetime.timedelta(days=((i+1)*length+shift))\n",
    "        else:\n",
    "            leftdate = startdate + datetime.timedelta(days=i*length)\n",
    "            rightdate = enddate\n",
    "        leftdate_str = datetime.date.strftime(leftdate, \"%m/%d/%Y\")\n",
    "        rightdate_str = datetime.date.strftime(rightdate, \"%m/%d/%Y\")\n",
    "        output_dates.append((leftdate_str, rightdate_str))\n",
    "    return output_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(keywords, time_periods, outname, MAX_PAGE=5, duplicate=True, pause=1):\n",
    "    # Download news by scrapping google search news section\n",
    "    results_all = []\n",
    "    count = 0\n",
    "    for leftdate, rightdate in time_periods:\n",
    "        # Duplicate will copy same article twice, and set one at 4am one at 12pm\n",
    "        googlenews = GoogleNews(lang='en', start=leftdate,end=rightdate, numperpage=50, duplicate=duplicate)\n",
    "        googlenews.search(keywords)\n",
    "        for i in range(2,MAX_PAGE+1):\n",
    "            time.sleep(pause)\n",
    "            googlenews.get_page(i)\n",
    "            if googlenews.failflag() == 1:\n",
    "                break\n",
    "        results_all.extend(googlenews.results())\n",
    "        print(\"Finish Request from {} to {}, Get {} articles\".format(leftdate, rightdate, len(googlenews.results())))\n",
    "        time.sleep(10)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            pd.DataFrame(results_all).to_csv(outname.format(keywords,count//10))\n",
    "            print(\"Reach 10 requests capacity, will sleep for 2000s. Last batch has been saved as period {}\".format(count//10))\n",
    "            time.sleep(2000)\n",
    "    return results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_cw(keywords, time_periods, MAX_PAGE=5):\n",
    "    # Download news by scrapping google search news section\n",
    "    results_all = []\n",
    "    \n",
    "    for leftdate, rightdate in time_periods:\n",
    "        results_period = []\n",
    "        \n",
    "        for i in range(1,MAX_PAGE+1):\n",
    "            tmp_results, fail_flag = fetch_context_web(keywords, leftdate, rightdate)\n",
    "            results_period.extend(tmp_results)\n",
    "            if fail_flag == 1:\n",
    "                break\n",
    "        results_all.extend(results_period)\n",
    "        print(\"Finish Request from {} to {}, Get {} articles\".format(leftdate, rightdate, len(results_period)))\n",
    "    return results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeperiods = generate_periods(startdate, enddate, length=30, shift=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "Scrapping google search has some risks. **The best practice is downloading 10 timeperiods at one time with `get_news`, wait for about half an hour, do another 10 timeperiods.**\n",
    "\n",
    "At maximum, as I tried, we can keep downloading 14 timepriods, then the IP will be blocked for about 2 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Bitcoin news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Bitcoin'\n",
    "output_file_name= \"../data/news/data/GoogleNews_{}_large_period_{}.csv\"\n",
    "MAX_PAGE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danger!!! This cell will make Google Search block your IP for about 2 hours\n",
    "Bicoin_news = get_news(keyword,timeperiods[:16], MAX_PAGE=MAX_PAGE, duplicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(Bicoin_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"../data/news/data/GoogleNews_{}_large_section1.csv\".format(keyword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have reached 06/24/2017 to 07/23/2017, should start from 07/24/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('07/24/2017', '08/22/2017')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeperiods[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not iterable\n",
      "Finish Request from 07/24/2017 to 08/22/2017, Get 356 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 08/23/2017 to 09/21/2017, Get 390 articles\n",
      "Finish Request from 09/22/2017 to 10/21/2017, Get 448 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 10/22/2017 to 11/20/2017, Get 290 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 11/21/2017 to 12/20/2017, Get 250 articles\n",
      "Finish Request from 12/21/2017 to 01/19/2018, Get 416 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 01/20/2018 to 02/18/2018, Get 338 articles\n",
      "Finish Request from 02/19/2018 to 03/20/2018, Get 442 articles\n",
      "Finish Request from 03/21/2018 to 04/19/2018, Get 492 articles\n",
      "Finish Request from 04/20/2018 to 05/19/2018, Get 448 articles\n",
      "Reach 10 requests capacity, will sleep for 2000s. Last batch has been saved as period 1\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 05/20/2018 to 06/18/2018, Get 344 articles\n",
      "Finish Request from 06/19/2018 to 07/18/2018, Get 466 articles\n",
      "Finish Request from 07/19/2018 to 08/17/2018, Get 420 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 08/18/2018 to 09/16/2018, Get 372 articles\n",
      "Finish Request from 09/17/2018 to 10/16/2018, Get 498 articles\n",
      "Finish Request from 10/17/2018 to 11/15/2018, Get 428 articles\n",
      "Finish Request from 11/16/2018 to 12/15/2018, Get 440 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 12/16/2018 to 01/14/2019, Get 392 articles\n",
      "Finish Request from 01/15/2019 to 02/13/2019, Get 500 articles\n",
      "Finish Request from 02/14/2019 to 03/15/2019, Get 486 articles\n",
      "Reach 10 requests capacity, will sleep for 2000s. Last batch has been saved as period 2\n",
      "Finish Request from 03/16/2019 to 04/14/2019, Get 500 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 04/15/2019 to 05/14/2019, Get 390 articles\n",
      "Finish Request from 05/15/2019 to 06/13/2019, Get 442 articles\n",
      "Finish Request from 06/14/2019 to 07/13/2019, Get 412 articles\n",
      "Finish Request from 07/14/2019 to 08/12/2019, Get 444 articles\n",
      "Finish Request from 08/13/2019 to 09/11/2019, Get 498 articles\n",
      "Finish Request from 09/12/2019 to 10/11/2019, Get 492 articles\n",
      "Finish Request from 10/12/2019 to 11/10/2019, Get 426 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 11/11/2019 to 12/10/2019, Get 368 articles\n",
      "Finish Request from 12/11/2019 to 01/09/2020, Get 482 articles\n",
      "Reach 10 requests capacity, will sleep for 2000s. Last batch has been saved as period 3\n",
      "Finish Request from 01/10/2020 to 02/08/2020, Get 492 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 02/09/2020 to 03/09/2020, Get 370 articles\n",
      "Finish Request from 03/10/2020 to 04/08/2020, Get 500 articles\n",
      "Finish Request from 04/09/2020 to 05/08/2020, Get 468 articles\n",
      "Finish Request from 05/09/2020 to 06/07/2020, Get 456 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 06/08/2020 to 07/07/2020, Get 378 articles\n",
      "Finish Request from 07/08/2020 to 08/06/2020, Get 434 articles\n",
      "Finish Request from 08/07/2020 to 09/05/2020, Get 472 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 09/06/2020 to 10/05/2020, Get 386 articles\n",
      "Finish Request from 10/06/2020 to 11/04/2020, Get 498 articles\n",
      "Reach 10 requests capacity, will sleep for 2000s. Last batch has been saved as period 4\n",
      "Finish Request from 11/05/2020 to 12/04/2020, Get 424 articles\n",
      "Finish Request from 12/05/2020 to 01/03/2021, Get 470 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 01/04/2021 to 02/02/2021, Get 380 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 02/03/2021 to 03/04/2021, Get 338 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 03/05/2021 to 04/03/2021, Get 54 articles\n",
      "'NoneType' object is not iterable\n",
      "Finish Request from 04/04/2021 to 04/16/2021, Get 274 articles\n"
     ]
    }
   ],
   "source": [
    "# This is a safe cell with \"appropriate\" wait time\n",
    "time.sleep(2000)\n",
    "Bitcoin_news2 = get_news(keyword,timeperiods[16:], outname=output_file_name, MAX_PAGE=MAX_PAGE, duplicate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "It turns out that, on Google search news section, when the publish date is too close to the current date (like 1-2 months ago), the date will show as something like \"1 month ago\", \"3 weeks ago\", et al. So it is hard to get exact publish date through current scrap tools. **The best practice is to stop at some time earlier and use another News API to download recent articles.** \n",
    "\n",
    "So I will drop articles later than 03/04/2021 and download articles from 03/05/2021 to 04/16/2021 by News APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-03-04 12:00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.DataFrame(Bitcoin_news2)\n",
    "df_all_short = df_all[df_all.datetime<=datetime.datetime(2021,3,5,0,0)]\n",
    "df_all_short.datetime.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_short.to_csv(\"../data/news/data/GoogleNews_Bitcoin_large_section_left.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use context web search API to download recent News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_periods = generate_periods(datetime.date(2021,3,5), datetime.date(2021,4,17), length=10, shift=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('03/05/2021', '03/15/2021'),\n",
       " ('03/15/2021', '03/25/2021'),\n",
       " ('03/25/2021', '04/04/2021'),\n",
       " ('04/04/2021', '04/14/2021'),\n",
       " ('04/14/2021', '04/17/2021')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Request from 03/05/2021 to 03/15/2021, Get 42 articles\n",
      "Finish Request from 03/15/2021 to 03/25/2021, Get 36 articles\n",
      "Finish Request from 03/25/2021 to 04/04/2021, Get 250 articles\n",
      "Finish Request from 04/04/2021 to 04/14/2021, Get 250 articles\n",
      "Finish Request from 04/14/2021 to 04/17/2021, Get 250 articles\n"
     ]
    }
   ],
   "source": [
    "Bitcoin_news6 = get_news_cw(keyword, recent_periods, MAX_PAGE=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame(Bitcoin_news6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_short = df6[df6.datetime < datetime.datetime(2021,4,17,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-04-16 20:37:04')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6_short.datetime.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_short.to_csv(\"../data/news/data/GoogleNews_{}_large_section6.csv\".format(keyword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the news from different sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"../data/news/data/GoogleNews_{}_large_section*\".format(keyword))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/news/data/GoogleNews_Bitcoin_large_section6.csv',\n",
       " '../data/news/data/GoogleNews_Bitcoin_large_section1.csv',\n",
       " '../data/news/data/GoogleNews_Bitcoin_large_section_left.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateframe_all = pd.read_csv(files[0], index_col=0)\n",
    "for file in files[1:]:\n",
    "    dateframe_all = pd.concat([dateframe_all,pd.read_csv(file, index_col=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateframe_all.sort_values(by=['datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateframe_all.to_csv(\"../data/news/data/GoogleNews_Bitcoin_large_all.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "spider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
